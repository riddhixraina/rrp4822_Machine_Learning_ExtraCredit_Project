# -*- coding: utf-8 -*-
"""Machine Learning Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CHBhk_8pDaqkGQ7TIGYckgnO2K2Lp9U0
"""



"""**Part 1: Data Collection & Preprocessing (15%)**

"""

## Setup and Installations
!pip install torch torchvision torchaudio matplotlib tqdm datasets transformers==4.44.2 music21==9.1.0 requests pandas

import os, random, time, json, math
import torch, torch.nn as nn
from torch.utils.data import IterableDataset, DataLoader
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import requests
import json
from pathlib import Path

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

## Configuration
class Cfg:
    DATASET = "the_session"  # or "lakh_midi" if you have it
    MAX_TUNES = 40000         # set None for all available
    SEQ_LEN = 512
    BATCH_TOKENS = 24576  # or 16384 instead of 32768
    TRAIN_TOKENS_TARGET = 100_000_000  # 100M tokens for scaling study
    VAL_FRAC = 0.01
    TEST_FRAC = 0.01
    LR = 3e-4
    WARMUP_STEPS = 100
    WEIGHT_DECAY = 0.1
    BETAS = (0.9, 0.95)
    CLIP_NORM = 1.0
    SAVE_DIR = "./runs"

cfg_main = Cfg()
os.makedirs(cfg_main.SAVE_DIR, exist_ok=True)
os.makedirs("./data", exist_ok=True)

## Download Data from The Session (REAL URL)
def download_session_abc():
    """
    Download ABC tunes from The Session via GitHub mirror.
    Real URL: https://github.com/adactio/TheSession-data
    """
    print("Downloading ABC tunes from The Session dataset...")

    # The Session data is available on GitHub
    url = "https://raw.githubusercontent.com/adactio/TheSession-data/master/json/tunes.json"

    try:
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        data = json.loads(response.text)

        # Extract ABC notation from each tune with data cleaning
        abc_texts = []
        invalid_count = 0
        too_short = 0
        too_long = 0

        MIN_LENGTH = 50   # Minimum characters per tune (filter very short)
        MAX_LENGTH = 50000 # Maximum characters per tune (filter very long)

        for tune in tqdm(data, desc="Extracting ABC"):
            if 'abc' not in tune or not tune['abc']:
                invalid_count += 1
                continue

            abc_text = tune['abc'].strip()

            # Filter very short sequences (likely corrupted/incomplete)
            if len(abc_text) < MIN_LENGTH:
                too_short += 1
                continue

            # Filter very long sequences (likely concatenated or corrupted)
            if len(abc_text) > MAX_LENGTH:
                too_long += 1
                continue

            # Basic validation: check if it looks like ABC notation
            # ABC files typically start with header fields like X:, T:, M:, K:
            if not any(abc_text.startswith(prefix) for prefix in ['X:', 'T:', 'M:', 'K:', 'C:']):
                # Might still be valid ABC, but less likely - keep for now
                pass

            abc_texts.append(abc_text)

        print(f"Downloaded {len(abc_texts)} valid ABC tunes")
        print(f"  - Invalid/missing: {invalid_count}")
        print(f"  - Too short (<{MIN_LENGTH} chars): {too_short}")
        print(f"  - Too long (>{MAX_LENGTH} chars): {too_long}")
        print(f"Total characters: {sum(len(t) for t in abc_texts):,}")
        return abc_texts

    except Exception as e:
        print(f"Error downloading: {e}")
        print("\nTrying alternative: Direct ABC file download...")

        # Alternative: Download ABC files directly
        # The Session provides ABC files at: http://abcnotation.com/tunesDB
        # But we'll use a simpler approach - download a pre-processed collection

        # Fallback: Use a smaller collection
        alt_url = "https://raw.githubusercontent.com/adactio/TheSession-data/master/json/tunes.json"
        try:
            response = requests.get(alt_url, timeout=60)
            data = json.loads(response.text)
            abc_texts = [t['abc'] for t in data if 'abc' in t and t['abc']]
            print(f"Downloaded {len(abc_texts)} tunes (alternative method)")
            return abc_texts
        except:
            print("Both download methods failed. Using minimal sample data.")
            # Minimal sample for testing
            return ["""X:1
T:Sample
M:4/4
K:C
C D E F | G A B c |"""] * 100

# Download the data
texts = download_session_abc()

# Shuffle for random train/val/test split
random.seed(42)
random.shuffle(texts)

# Limit if specified
if cfg_main.MAX_TUNES:
    texts = texts[:cfg_main.MAX_TUNES]

# Calculate total tokens (characters ≈ tokens for char-level tokenization)
total_chars = sum(len(t) for t in texts)
seq_lengths = [len(t) for t in texts]

print(f"\n{'='*60}")
print(f"DATA PREPROCESSING SUMMARY")
print(f"{'='*60}")
print(f"Final dataset size: {len(texts)} tunes")
print(f"Total characters (≈tokens): {total_chars:,}")
print(f"Target training tokens: {cfg_main.TRAIN_TOKENS_TARGET:,}")

# Sequence length statistics
import numpy as np
print(f"\nSequence Length Distribution:")
print(f"  Min: {min(seq_lengths):,} chars")
print(f"  Max: {max(seq_lengths):,} chars")
print(f"  Mean: {np.mean(seq_lengths):.1f} chars")
print(f"  Median: {np.median(seq_lengths):.1f} chars")
print(f"  Std: {np.std(seq_lengths):.1f} chars")

# Document filtering criteria
print(f"\nData Cleaning Applied:")
print(f"  ✓ Removed tunes with missing/invalid ABC field")
print(f"  ✓ Filtered tunes < 50 characters (too short/incomplete)")
print(f"  ✓ Filtered tunes > 50,000 characters (likely corrupted/concatenated)")
print(f"  ✓ Very long sequences will be chunked into {cfg_main.SEQ_LEN}-token segments during training")
print(f"  ✓ Very short sequences are automatically skipped (need {cfg_main.SEQ_LEN}+1 tokens for a sample)")

if total_chars < cfg_main.TRAIN_TOKENS_TARGET:
    cycles_needed = cfg_main.TRAIN_TOKENS_TARGET / total_chars
    print(f"\n⚠️  Dataset has {total_chars:,} tokens, need {cfg_main.TRAIN_TOKENS_TARGET:,}")
    print(f"   Will cycle through data ~{cycles_needed:.1f} times to reach target")
else:
    print(f"\n✓ Dataset has enough tokens ({total_chars:,} >= {cfg_main.TRAIN_TOKENS_TARGET:,})")

print(f"\nSample ABC (first 500 chars):\n{texts[0][:500]}")

## Tokenizer (Character-level)
"""
Tokenization Strategy: Character-level

Choice: Character-level tokenization
Rationale:
  - Simple and robust: no out-of-vocabulary (OOV) issues
  - Preserves all ABC notation syntax exactly (spaces, newlines, special chars)
  - Works well for structured text like ABC notation
  - Easy to implement and debug
  - Vocabulary size is small (~100-200 chars) vs note-level (thousands)

Alternatives considered:
  - Note-level: Would tokenize into musical events (e.g., "C4", "D4", "|")
    * Pros: More musically meaningful tokens
    * Cons: Larger vocabulary, more complex, potential OOV issues
  - Custom music-aware: Would parse ABC into structured tokens
    * Pros: Most musically aware
    * Cons: Complex, requires ABC parser, loses some flexibility

For this scaling study, character-level is ideal because:
  1. Fair comparison across model sizes (same tokenization)
  2. Robust to dataset variations
  3. Focus is on scaling laws, not optimal tokenization
"""

class CharTokenizer:
    def __init__(self, texts):
        # Collect all unique characters
        chars = sorted(set("".join(texts)))
        self.itos = ["<pad>", "<bos>", "<eos>"] + chars
        self.stoi = {c: i for i, c in enumerate(self.itos)}
        self.pad_id, self.bos_id, self.eos_id = 0, 1, 2

    def encode(self, s):
        return [self.bos_id] + [self.stoi.get(ch, self.pad_id) for ch in s] + [self.eos_id]

    def decode(self, ids):
        rev = {i: c for i, c in enumerate(self.itos)}
        return "".join(rev[i] for i in ids if i >= 3)

tokenizer = CharTokenizer(texts)
vocab_size = len(tokenizer.itos)
print(f"{'='*60}")
print(f"TOKENIZATION SUMMARY")
print(f"{'='*60}")
print(f"Strategy: Character-level tokenization")
print(f"Vocabulary size: {vocab_size}")
print(f"Special tokens: <pad> (0), <bos> (1), <eos> (2)")
print(f"Character tokens: {vocab_size - 3}")
print(f"\nSample tokens: {tokenizer.itos[:30]}...")
print(f"  (Full vocabulary includes all ASCII chars, newlines, spaces, etc.)")

# Streaming Dataset with Train/Val/Test Splits
class ABCStream(IterableDataset):
    def __init__(self, texts, tokenizer, split):
        n = len(texts)
        val_n = int(n * cfg_main.VAL_FRAC)
        test_n = int(n * cfg_main.TEST_FRAC)
        train_n = n - val_n - test_n

        self.split_ranges = {
            "train": (0, train_n),
            "val":   (train_n, train_n + val_n),
            "test":  (train_n + val_n, n),
        }
        self.texts = texts
        self.tokenizer = tokenizer
        self.split = split

        # For training: track tokens to ensure we hit target
        if split == "train":
            # Estimate tokens per tune (rough estimate: chars + bos/eos)
            avg_tokens_per_tune = sum(len(t) + 2 for t in texts[:min(100, len(texts))]) / min(100, len(texts))
            self.estimated_tokens = int(avg_tokens_per_tune * train_n)
            self.target_tokens = cfg_main.TRAIN_TOKENS_TARGET
        else:
            self.estimated_tokens = None
            self.target_tokens = None

    def __iter__(self):
        lo, hi = self.split_ranges[self.split]
        split_texts = self.texts[lo:hi]

        # For training: cycle through data until we hit target tokens
        if self.split == "train" and self.target_tokens:
            tokens_generated = 0
            cycle = 0

            while tokens_generated < self.target_tokens:
                cycle += 1
                # Reshuffle each cycle for randomness
                idxs = list(range(len(split_texts)))
                random.shuffle(idxs)

                buf = []
                for i in idxs:
                    ids = self.tokenizer.encode(split_texts[i])
                    buf.extend(ids)
                    tokens_generated += len(ids)

                    while len(buf) >= cfg_main.SEQ_LEN + 1:
                        x = buf[:cfg_main.SEQ_LEN]
                        y = buf[1:cfg_main.SEQ_LEN + 1]
                        buf = buf[cfg_main.SEQ_LEN:]
                        yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

                    # Stop if we've hit target
                    if tokens_generated >= self.target_tokens:
                        break

                if cycle == 1:
                    print(f"  Completed cycle {cycle}, ~{tokens_generated:,} tokens generated")
                elif cycle % 5 == 0:
                    print(f"  Completed cycle {cycle}, ~{tokens_generated:,} tokens generated")
        else:
            # For val/test: just go through once
            idxs = list(range(len(split_texts)))
            random.shuffle(idxs)

            buf = []
            for i in idxs:
                ids = self.tokenizer.encode(split_texts[i])
                buf.extend(ids)

                while len(buf) >= cfg_main.SEQ_LEN + 1:
                    x = buf[:cfg_main.SEQ_LEN]
                    y = buf[1:cfg_main.SEQ_LEN + 1]
                    buf = buf[cfg_main.SEQ_LEN:]
                    yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)

def make_loader(split):
    batch = cfg_main.BATCH_TOKENS // cfg_main.SEQ_LEN
    return DataLoader(
        ABCStream(texts, tokenizer, split),
        batch_size=batch,
        num_workers=0,  # Colab works better with 0
        pin_memory=True if device == "cuda" else False
    )

train_loader = make_loader("train")
val_loader = make_loader("val")
test_loader = make_loader("test")

print(f"Batch size (number of sequences): {cfg_main.BATCH_TOKENS // cfg_main.SEQ_LEN}")
print(f"Tokens per batch: {cfg_main.BATCH_TOKENS}")
print(f"\n✓ Training loader will cycle through data until {cfg_main.TRAIN_TOKENS_TARGET:,} tokens")
print(f"  This ensures all models train on the same amount of data for fair comparison")

"""**Part 2: Transformer Scaling Study (40%)**"""

## Model Definitions - Transformer
"""
PART 2: Transformer Scaling Study - Model Architecture

This cell implements a decoder-only transformer language model (GPT-style).
This architecture is required for Part 2: "Train a family of decoder-only
transformer language models of varying sizes."

Architecture details:
- Decoder-only: Uses causal attention mask (no encoder)
- GPT-style: Pre-norm architecture with layer normalization before attention/FFN
- Standard components: Multi-head attention, feed-forward network, residual connections

Reference: Based on nanoGPT architecture, adapted for music ABC notation.
"""
class GPTBlock(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, attn_p=0.1, ff_p=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=attn_p, batch_first=True)
        self.ln2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(ff_p),
            nn.Linear(d_ff, d_model),
            nn.Dropout(ff_p)
        )

    def forward(self, x, attn_mask):
        h = self.ln1(x)
        a, _ = self.attn(h, h, h, attn_mask=attn_mask, need_weights=False)
        x = x + a
        h = self.ln2(x)
        x = x + self.ff(h)
        return x

class MiniGPT(nn.Module):
    def __init__(self, vocab, d_model, n_layers, n_heads, d_ff):
        super().__init__()
        self.emb = nn.Embedding(vocab, d_model)
        self.pos = nn.Embedding(cfg_main.SEQ_LEN, d_model)
        self.blocks = nn.ModuleList([
            GPTBlock(d_model, n_heads, d_ff) for _ in range(n_layers)
        ])
        self.ln_f = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, vocab, bias=False)
        self.register_buffer("mask", torch.triu(torch.ones(cfg_main.SEQ_LEN, cfg_main.SEQ_LEN) * float("-inf"), 1))

    def forward(self, x):
        B, T = x.size()
        h = self.emb(x) + self.pos.weight[:T]
        mask = self.mask[:T, :T]
        for blk in self.blocks:
            h = blk(h, mask)
        h = self.ln_f(h)
        return self.head(h)

## Model Configurations
"""
PART 2: Model Size Configurations

This cell defines 5 different model sizes as required by Part 2:
"Train at least 5 different model sizes. Suggested configurations:
• Tiny: ~1M parameters
• Small: ~5M parameters
• Medium: ~20M parameters
• Large: ~50M parameters
• XL: ~100M+ parameters"

Each configuration varies hyperparameters (n_layers, n_heads, d_model, d_ff)
as specified in the requirements: "Vary hyperparameters like: n layers,
n heads, d model, d ff to achieve different parameter counts."

The actual parameter counts will be calculated during training and may vary
slightly from targets based on vocabulary size and exact architecture.
"""
# def tcfgs():
#     """
#     Transformer configurations for scaling study (T4-friendly).
#     Note: For the largest configs (xxl/xxxl), reduce BATCH_TOKENS to ~24k
#     (or lower) before running to avoid OOM on a T4.
#     """
#     return [
#         ("tiny",   {"d_model": 128, "n_layers": 2,  "n_heads": 4, "d_ff": 512}),    # ~1M
#         ("small",  {"d_model": 192, "n_layers": 3,  "n_heads": 4, "d_ff": 768}),    # ~5M
#         ("medium", {"d_model": 256, "n_layers": 4,  "n_heads": 4, "d_ff": 1024}),   # ~10M
#         ("large",  {"d_model": 320, "n_layers": 6,  "n_heads": 5, "d_ff": 1280}),   # ~15M
#         ("xl",     {"d_model": 384, "n_layers": 8,  "n_heads": 6, "d_ff": 1536}),   # ~14–15M (current top end)
#         # Bigger (optional) — lower batch tokens to ~24k for these:
#         ("xxl",    {"d_model": 448, "n_layers": 10, "n_heads": 7, "d_ff": 1792}),   # ~22–25M
#         ("xxxl",   {"d_model": 512, "n_layers": 12, "n_heads": 8, "d_ff": 2048}),   # ~40–50M (may need 20–24k tokens)
#     ]
def tcfgs():
    return [
        ("tiny",   {"d_model": 128, "n_layers": 2,  "n_heads": 4,  "d_ff": 512}),
        ("small",  {"d_model": 192, "n_layers": 3,  "n_heads": 4,  "d_ff": 768}),
        ("medium", {"d_model": 256, "n_layers": 4,  "n_heads": 4,  "d_ff": 1024}),
        ("large",  {"d_model": 320, "n_layers": 6,  "n_heads": 5,  "d_ff": 1280}),
        ("xl",     {"d_model": 768, "n_layers": 12, "n_heads": 12, "d_ff": 3072}),  # ~100M+ parameters on our vocab
        #A100 + time:
        ("xxl",  {"d_model": 704, "n_layers": 14, "n_heads": 11, "d_ff": 2816}),
    ]

def lcfgs():
    """
    LSTM configurations for scaling study (T4-friendly).
    For the largest LSTMs, reduce BATCH_TOKENS to ~24k if memory is tight.
    """
    return [
        ("tiny_rnn",   {"d_model": 128, "n_layers": 1}),
        ("small_rnn",  {"d_model": 192, "n_layers": 1}),
        ("med_rnn",    {"d_model": 256, "n_layers": 2}),
        ("large_rnn",  {"d_model": 320, "n_layers": 2}),
        # Larger LSTMs to widen parameter range
        ("xl_rnn",     {"d_model": 384, "n_layers": 3}),   # ~6–8M
        ("xxl_rnn",    {"d_model": 448, "n_layers": 3}),   # ~10–14M
        ("xxxl_rnn",   {"d_model": 512, "n_layers": 4}),   # ~20–30M (lower batch tokens if needed)
        ("xxxxl_rnn",  {"d_model": 640, "n_layers": 4})  # rough ~50–70M params depending on vocab
    ]

## LSTM Model
class MiniLSTM(nn.Module):
    def __init__(self, vocab, d_model, n_layers):
        super().__init__()
        self.emb = nn.Embedding(vocab, d_model)
        self.lstm = nn.LSTM(d_model, d_model, num_layers=n_layers, batch_first=True)
        self.head = nn.Linear(d_model, vocab)

    def forward(self, x):
        h = self.emb(x)
        out, _ = self.lstm(h)
        return self.head(out)

## Training Utilities
"""
PART 2: Training Utilities with Metrics Tracking

This cell implements training functions that track all required metrics:
1. Training loss curves over time
2. Wall-clock time per epoch
3. GPU memory usage
4. Final training and validation loss

All models use consistent training setup as required:
- Same tokenization scheme (from Cell 4)
- Same learning rate schedule (cfg.LR)
- Same batch size measured in tokens (cfg_main.BATCH_TOKENS)
- Same training data (100M tokens via train_loader)
- Train for exactly 1 epoch (for scaling plot comparison)
"""

def count_params(m):
    """Count total parameters in model - used for scaling plot x-axis"""
    return sum(p.numel() for p in m.parameters())

def run_epoch(model, loader, optimizer=None, scaler=None, track_curve=False):
    """
    Run one epoch of training or validation.

    PART 2 Requirement: "Track additional metrics: Training loss curves over time"
    - If track_curve=True, samples loss values during training for plotting curves
    - This satisfies: "Training loss curves over time for each model"

    Args:
        model: Model to train/validate
        loader: DataLoader for training or validation
        optimizer: Optimizer (None for validation)
        scaler: Mixed precision scaler (None for validation)
        track_curve: If True, return loss curve for plotting (Part 2 requirement)

    Returns:
        If track_curve: (avg_loss, loss_curve) where loss_curve is [(step, loss), ...]
        Otherwise: avg_loss (float)
    """
    is_train = optimizer is not None
    model.train(is_train)
    total_loss, total_tokens = 0, 0
    loss_curve = [] if track_curve else None
    # For iterable datasets we do not know len(loader); sample every N steps
    log_every = 100  # adjust if you want denser curves

    for step, (xb, yb) in enumerate(tqdm(loader, desc="Training" if is_train else "Validation", leave=False)):
        xb, yb = xb.to(device), yb.to(device)

        if is_train:
            optimizer.zero_grad(set_to_none=True)

        # Forward pass with mixed precision (new API to avoid deprecation warnings)
        autocast_enabled = scaler is not None and device == "cuda"
        with torch.amp.autocast("cuda", enabled=autocast_enabled):
            logits = model(xb)
            loss = nn.functional.cross_entropy(
                logits.view(-1, vocab_size),
                yb.view(-1)
            )

        if is_train:
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg_main.CLIP_NORM)
            scaler.step(optimizer)
            scaler.update()

        # Accumulate loss (weighted by tokens for accurate average)
        tokens = xb.numel()
        total_loss += loss.item() * tokens
        total_tokens += tokens

        # PART 2 Requirement: Track training loss curves over time
        # For iterable datasets, sample every `log_every` steps
        if track_curve and is_train and (step % log_every == 0):
            loss_curve.append((step, loss.item()))

    avg_loss = total_loss / total_tokens
    return (avg_loss, loss_curve) if track_curve else avg_loss

def train_one_epoch(model, optimizer, train_loader, val_loader, track_curve=False):
    """
    Train for exactly 1 epoch and return all required metrics.

    PART 2 Requirements:
    1. "Train for exactly 1 epoch for the scaling plot comparison" ✓
    2. "Wall-clock time per epoch" ✓ (tracked via time.time())
    3. "GPU memory usage" ✓ (tracked via torch.cuda.max_memory_allocated())
    4. "Training loss curves over time" ✓ (if track_curve=True)
    5. "Measure the validation loss after 1 epoch" ✓ (returned as val_loss)

    This function ensures consistent training setup across all models:
    - Same tokenization (already applied in data)
    - Same learning rate (cfg.LR)
    - Same batch size in tokens (cfg.BATCH_TOKENS)
    - Same training data (100M tokens via train_loader)
    - Exactly 1 epoch (this function runs once per model)

    Returns:
        train_loss: Final training loss (float)
        val_loss: Validation loss after 1 epoch (float) - REQUIRED for scaling plot
        wall_time: Wall-clock time in seconds (float)
        train_curve: Training loss curve [(step, loss), ...] or None
        mem_peak: Peak GPU memory usage in GB (float)
    """
    # Use new API to avoid deprecation warning
    scaler = torch.amp.GradScaler("cuda") if device == "cuda" else torch.amp.GradScaler("cpu")
    t0 = time.time()  # PART 2: Track wall-clock time per epoch

    # Track GPU memory usage
    if device == "cuda":
        torch.cuda.reset_peak_memory_stats()  # Reset before training
        mem_before = torch.cuda.memory_allocated() / 1e9  # GB

    # Run training epoch (with curve tracking if requested)
    # Track training loss curves over time
    if track_curve:
        train_loss, train_curve = run_epoch(model, train_loader, optimizer, scaler, track_curve=True)
    else:
        train_loss = run_epoch(model, train_loader, optimizer, scaler, track_curve=False)
        train_curve = None

    #  "Measure the validation loss after 1 epoch"
    # This val_loss is used for the scaling plot (y-axis)
    val_loss = run_epoch(model, val_loader, optimizer=None, scaler=None, track_curve=False)

    # Track GPU memory usage
    if device == "cuda":
        mem_peak = torch.cuda.max_memory_allocated() / 1e9  # GB - peak during training
        mem_after = torch.cuda.memory_allocated() / 1e9  # GB - current after training
    else:
        mem_before = mem_peak = mem_after = 0.0

    # Wall-clock time per epoch
    wall_time = time.time() - t0

    if track_curve:
        return train_loss, val_loss, wall_time, train_curve, mem_peak
    else:
        return train_loss, val_loss, wall_time, None, mem_peak

## Run Transformer Scaling Study
"""
PART 2: Transformer Scaling Study - Main Training Loop

This cell implements the core requirement: "Train a family of decoder-only
transformer language models of varying sizes. Measure the validation loss
after 1 epoch of training for each model size."

Requirements satisfied:
1. Train at least 5 different model sizes ✓ (tiny, small, medium, large, xl)
2. Use consistent training setup across all models ✓
   - Same tokenization scheme ✓
   - Same learning rate schedule ✓
   - Same batch size (measured in tokens) ✓
   - Same training data (at least 100M tokens) ✓
   - Train for exactly 1 epoch ✓
3. Track additional metrics:
   - Training loss curves over time ✓
   - Wall-clock time per epoch ✓
   - GPU memory usage ✓
4. Measure validation loss after 1 epoch ✓ (used for scaling plot)

The results are stored in 'results' list and 'training_curves' dict for:
- Scaling plot (Cell 12): validation loss vs model size
- Training curves plot (Cell 12b): loss over time
- Architecture table (Cell 12c): all statistics
"""

results = []  # Store all training results for scaling analysis
training_curves = {}  # Store training loss curves for plotting

def run_family_gpt():
    """
    Train 5 transformer models - exactly 1 epoch each.

    "Train at least 5 different model sizes"
    This function trains all 5 models (tiny, small, medium, large, xl) with
    consistent setup and tracks all required metrics.
    """
    for name, cfgm in tcfgs():
        torch.cuda.empty_cache() if device == "cuda" else None

        model = MiniGPT(vocab_size, **cfgm).to(device)
        params = count_params(model)

        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=cfg_main.LR,
            betas=cfg_main.BETAS,
            weight_decay=cfg_main.WEIGHT_DECAY
        )

        print(f"\n{'='*60}")
        print(f"Training GPT: {name} ({params/1e6:.2f}M parameters)")
        print(f"{'='*60}")

        # Train for exactly 1 epoch with all metrics tracked
        # train_one_epoch() ensures consistent setup:
        # - Same tokenization (from Cell 4)
        # - Same learning rate (cfg_main.LR)
        # - Same batch size in tokens (cfg_main.BATCH_TOKENS)
        # - Same training data (100M tokens via train_loader)
        # - Exactly 1 epoch (as required for scaling plot)
        train_loss, val_loss, wall, train_curve, mem_peak = train_one_epoch(
            model, optimizer, train_loader, val_loader, track_curve=True
        )

        #  Store all required metrics for scaling analysis
        results.append({
            "model": name,
            "kind": "gpt",
            "params": params,              # For scaling plot x-axis (log scale)
            "train_loss": train_loss,       # Final training loss
            "val_loss": val_loss,           # REQUIRED: Validation loss after 1 epoch (for scaling plot y-axis)
            "time_s": wall,                 # Wall-clock time per epoch
            "gpu_memory_gb": mem_peak       # GPU memory usage
        })

        # "Training loss curves over time for each model"
        # Store curve for plotting in Cell 12b
        if train_curve:
            training_curves[f"gpt_{name}"] = train_curve

        print(f"✓ {name}: params={params/1e6:.2f}M, train={train_loss:.3f}, val={val_loss:.3f}")
        print(f"  Time: {wall/60:.1f}m, GPU Memory: {mem_peak:.2f}GB")

        torch.save(model.state_dict(), f"{cfg_main.SAVE_DIR}/gpt_{name}.pt")
        del model, optimizer
        torch.cuda.empty_cache() if device == "cuda" else None

# Run the scaling study - trains all 5 transformer models
# Note: For larger configs (xxl/xxxl), consider lowering cfg.BATCH_TOKENS to ~24k (or 20k) to avoid OOM on T4.
run_family_gpt()

# Save all results for analysis and report
# scaling_results.json: Contains all metrics (loss, time, memory, params) for scaling plot
json.dump(results, open(f"{cfg_main.SAVE_DIR}/scaling_results.json", "w"), indent=2)
# training_curves.json: Contains loss curves over time for plotting (Part 2 requirement)
json.dump(training_curves, open(f"{cfg_main.SAVE_DIR}/training_curves.json", "w"), indent=2)
print(f"\n✓ Saved results to {cfg_main.SAVE_DIR}/scaling_results.json")
print(f"✓ Saved training curves to {cfg_main.SAVE_DIR}/training_curves.json")

"""**Part 3: RNN Scaling Study and Comparison (20%)**"""

## Run LSTM Scaling Study (Part 3)
"""
PART 3: RNN Scaling Study - LSTM Training

This cell implements Part 3 requirement: "Repeat the scaling study using
RNN-based models (use LSTM)"

Requirements satisfied:
1. Train at least 4 different RNN sizes ✓ (tiny_rnn, small_rnn, med_rnn, large_rnn)
   - Similar parameter counts to transformer models ✓
2. Use the same data and training setup ✓
   - Same tokenization (from Cell 4) ✓
   - Same training data (100M tokens via train_loader) ✓
   - Same batch size (cfg_main.BATCH_TOKENS) ✓
   - Same learning rate (cfg_main.LR) ✓
   - Exactly 1 epoch ✓
3. Track all metrics for comparison:
   - Validation loss after 1 epoch ✓ (for scaling plot)
   - Training time ✓ (for efficiency analysis)
   - GPU memory ✓ (for efficiency analysis)
   - Training curves ✓ (for sample efficiency)

The results will be used in Cell 12 for:
- RNN scaling plot (Part 3 deliverable)
- Combined comparison plot (Part 3 deliverable)
- Efficiency analysis (Cell 12d)
"""

def run_family_lstm():
    """
    Train 4 LSTM models - exactly 1 epoch each.

    PART 3 Requirement: "Train at least 4 different RNN sizes with similar
    parameter counts to your transformer models"
    """
    for name, cfgm in lcfgs():
        torch.cuda.empty_cache() if device == "cuda" else None

        model = MiniLSTM(vocab_size, **cfgm).to(device)
        params = count_params(model)

        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=cfg_main.LR,
            betas=cfg_main.BETAS,
            weight_decay=cfg_main.WEIGHT_DECAY
        )

        print(f"\n{'='*60}")
        print(f"Training LSTM: {name} ({params/1e6:.2f}M parameters)")
        print(f"{'='*60}")

        # Train with curve tracking
        train_loss, val_loss, wall, train_curve, mem_peak = train_one_epoch(
            model, optimizer, train_loader, val_loader, track_curve=True
        )

        results.append({
            "model": name,
            "kind": "lstm",
            "params": params,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "time_s": wall,
            "gpu_memory_gb": mem_peak
        })

        # Store training curve
        if train_curve:
            training_curves[f"lstm_{name}"] = train_curve

        print(f"✓ {name}: params={params/1e6:.2f}M, train={train_loss:.3f}, val={val_loss:.3f}")
        print(f"  Time: {wall/60:.1f}m, GPU Memory: {mem_peak:.2f}GB")

        torch.save(model.state_dict(), f"{cfg_main.SAVE_DIR}/lstm_{name}.pt")
        del model, optimizer
        torch.cuda.empty_cache() if device == "cuda" else None

# PART 3: Run the LSTM scaling study - trains all 4 LSTM models
# Note: For larger configs (xxl/xxxl), consider lowering cfg.BATCH_TOKENS to ~24k (or 20k) to avoid OOM on T4.
run_family_lstm()

# PART 3: Save all results for comparison analysis
# Results are appended to the same results list as GPT models for easy comparison
json.dump(results, open(f"{cfg_main.SAVE_DIR}/scaling_results.json", "w"), indent=2)
json.dump(training_curves, open(f"{cfg_main.SAVE_DIR}/training_curves.json", "w"), indent=2)
print(f"\n✓ All results saved")

## Scaling Plots and Power Law Fit (Part 2 & Part 3)
"""
PART 2 & PART 3: Scaling Plots and Power Law Analysis

This cell implements:
- PART 2: "Create a scaling plot" for transformers
- PART 3: "Create an RNN scaling plot" and "Plot both scaling curves on the same graph"

Requirements:
1. X-axis: Model size (number of parameters, log scale) ✓
2. Y-axis: Validation loss after 1 epoch ✓
3. Fit a power law of the form: L = a · N^(-α) + c ✓
4. Report the fitted scaling exponent α ✓
5. Discuss its implications (in report)

The scaling plot shows how validation loss decreases as model size increases,
following a power law relationship. This is a fundamental scaling law for
neural language models (Kaplan et al., 2020).

The power law exponent α indicates how efficiently larger models improve:
- Higher α = steeper improvement with size
- Lower α = diminishing returns with size
"""

import numpy as np

def fit_power_law(xs, ys):
    """
    Fit power law: L = a * N^(-alpha) + c

    PART 2 Requirement: "Fit a power law of the form: L = a · N^(-α) + c
    where N is the parameter count"

    This function fits the scaling law to the data and returns:
    - a: Scaling coefficient
    - alpha: Scaling exponent (REQUIRED to report in Part 2)
    - c: Baseline loss (irreducible error)

    The exponent α quantifies how loss scales with model size and is a key
    metric for understanding scaling behavior.

    Args:
        xs: Model sizes (parameter counts)
        ys: Validation losses after 1 epoch

    Returns:
        a: Scaling coefficient
        alpha: Scaling exponent (negative of fitted slope)
        c: Baseline loss (minimum observed loss)
    """
    c = min(ys)  # Baseline loss (irreducible error)
    y_adj = np.array(ys) - c + 1e-8  # Adjust for log
    X = np.log(np.array(xs))  # Log of parameter counts
    Y = np.log(y_adj)  # Log of adjusted losses
    A = np.vstack([X, np.ones_like(X)]).T
    alpha, loga = np.linalg.lstsq(A, Y, rcond=None)[0]
    return np.exp(loga), -alpha, c

def plot_scaling(kind):
    """
    Create scaling plot with power law fit.

    PART 2 Deliverable: "Scaling plot with power law fit"

    Requirements satisfied:
    1. X-axis: Model size (number of parameters, log scale) ✓
    2. Y-axis: Validation loss after 1 epoch ✓
    3. Fit power law: L = a · N^(-α) + c ✓
    4. Report fitted scaling exponent α ✓

    This plot is the main deliverable for Part 2 and shows how validation
    loss scales with model size. The power law fit quantifies the relationship.

    Args:
        kind: "gpt" for transformers or "lstm" for RNNs

    Returns:
        alpha: Scaling exponent (for report discussion)
    """
    # Extract data: model sizes (x) and validation losses (y)
    xs = [r["params"] for r in results if r["kind"] == kind]
    ys = [r["val_loss"] for r in results if r["kind"] == kind]

    if not xs:
        print(f"No results for {kind}")
        return None

    # PART 2: Fit power law L = a * N^(-alpha) + c
    a, alpha, c = fit_power_law(xs, ys)

    # Generate smooth curve for plotting
    xp = np.logspace(np.log10(min(xs)), np.log10(max(xs)), 100)
    yp = a * xp**(-alpha) + c

    # PART 2: Create scaling plot
    plt.figure(figsize=(8, 6))
    plt.scatter(xs, ys, s=100, label=f"{kind.upper()} validation loss", zorder=3)
    plt.plot(xp, yp, '--', label=f"Power law fit: α={alpha:.3f}", linewidth=2)
    plt.xscale("log")  # PART 2: Log scale on x-axis (model size)
    plt.xlabel("Model Size (parameters)", fontsize=12)
    plt.ylabel("Validation Loss", fontsize=12)  # PART 2: Validation loss after 1 epoch
    plt.title(f"{kind.upper()} Scaling Law", fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # PART 2 Requirement: Report the fitted scaling exponent α
    print(f"{kind.upper()} scaling exponent α = {alpha:.3f}")
    print(f"  (Reported this value in Part 2 analysis)")
    return alpha

alpha_gpt = plot_scaling("gpt")
alpha_lstm = plot_scaling("lstm")

# PART 3: Combined comparison plot with power law fits
# Deliverable: "Plot both scaling curves on the same graph"
plt.figure(figsize=(10, 6))
for kind, marker, color in [("gpt", "o", "blue"), ("lstm", "s", "red")]:
    xs = [r["params"] for r in results if r["kind"] == kind]
    ys = [r["val_loss"] for r in results if r["kind"] == kind]
    if xs:
        # Plot data points
        plt.scatter(xs, ys, marker=marker, s=100, label=kind.upper(), color=color, zorder=3)

        # Plot power law fit
        a, alpha, c = fit_power_law(xs, ys)
        xp = np.logspace(np.log10(min(xs)), np.log10(max(xs)), 100)
        yp = a * xp**(-alpha) + c
        plt.plot(xp, yp, '--', color=color, alpha=0.7, linewidth=2,
                label=f"{kind.upper()} fit (α={alpha:.3f})")

plt.xscale("log")
plt.xlabel("Model Size (parameters)", fontsize=12)
plt.ylabel("Validation Loss", fontsize=12)
plt.title("PART 3: Transformer vs LSTM Scaling Comparison", fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nPART 3 Comparison:")
print(f"  Transformer scaling exponent: α = {alpha_gpt:.3f}")
print(f"  LSTM scaling exponent:        α = {alpha_lstm:.3f}")
if alpha_gpt > alpha_lstm:
    print(f"  → Transformer scales better (steeper exponent)")
else:
    print(f"  → LSTM scales better (steeper exponent)")

## Training Loss Curves
"""
PART 2: Training Loss Curves

This cell implements the requirement: "Track additional metrics:
Training loss curves over time for each model"

Deliverable: "Training curves for all models"

This plot shows how training loss decreases during the 1 epoch of training
for each model size. It helps visualize:
- Convergence speed (how quickly loss decreases)
- Training stability (smooth vs noisy curves)
- Relative performance across model sizes

The curves are normalized to 0-1 progress to compare across different
dataset sizes (since larger models may process data at different rates).
"""

# Plot training loss curves for all transformer models
plt.figure(figsize=(12, 6))

# Load curves if not in memory (in case you restart kernel)
if not training_curves:
    try:
        training_curves = json.load(open(f"{cfg_main.SAVE_DIR}/training_curves.json"))
    except:
        print("No training curves found. Re-run Cell 10 to generate curves.")
        training_curves = {}

# Plot curves for each GPT model
for name, _ in tcfgs():
    curve_key = f"gpt_{name}"
    if curve_key in training_curves:
        curve = training_curves[curve_key]
        steps, losses = zip(*curve) if curve else ([], [])
        if steps:
            # Normalize steps to 0-1 for comparison across different dataset sizes
            steps_norm = [s / max(steps) for s in steps]
            plt.plot(steps_norm, losses, label=f"{name}", linewidth=2, alpha=0.8)

plt.xlabel("Training Progress (normalized)", fontsize=12)
plt.ylabel("Training Loss", fontsize=12)
plt.title("Transformer Training Loss Curves (1 Epoch)", fontsize=14)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("Training curves show how loss decreases during training for each model size.")

## Model Architecture Table
"""
PART 2: Model Architecture Table

This cell creates the deliverable: "Table of model architectures and
training statistics"

The table includes:
1. Model architectures (d_model, n_layers, n_heads, d_ff) - shows how
   hyperparameters vary across model sizes
2. Parameter counts - for scaling plot x-axis
3. Training statistics:
   - Train loss (final)
   - Val loss (after 1 epoch) - for scaling plot y-axis
   - Wall-clock time per epoch - PART 2 requirement
   - GPU memory usage - PART 2 requirement

This table can be directly copied into your report for Part 2.
"""

# Create a table of model architectures and training statistics
import pandas as pd

# Filter GPT results (transformer models only for Part 2)
gpt_results = [r for r in results if r["kind"] == "gpt"]

# Get configs for each model to show architecture details
configs_dict = {name: cfgm for name, cfgm in tcfgs()}

# Create table with all required information
table_data = []
for r in gpt_results:
    name = r["model"]
    cfgm = configs_dict[name]
    table_data.append({
        "Model": name,  # Model size name (tiny, small, medium, large, xl)
        "Params (M)": f"{r['params']/1e6:.2f}",  # Total parameters (for scaling plot x-axis)
        # Architecture hyperparameters (Part 2: "Vary hyperparameters like: n layers, n heads, d model, d ff")
        "d_model": cfgm["d_model"],      # Model dimension
        "n_layers": cfgm["n_layers"],    # Number of transformer layers
        "n_heads": cfgm["n_heads"],       # Number of attention heads
        "d_ff": cfgm["d_ff"],            # Feed-forward dimension
        # Training statistics (Part 2 requirements)
        "Train Loss": f"{r['train_loss']:.3f}",  # Final training loss
        "Val Loss": f"{r['val_loss']:.3f}",      # PART 2: Validation loss after 1 epoch (for scaling plot y-axis)
        "Time (min)": f"{r['time_s']/60:.1f}",   # PART 2: Wall-clock time per epoch
        "GPU Mem (GB)": f"{r.get('gpu_memory_gb', 0):.2f}" if device == "cuda" else "N/A"  # PART 2: GPU memory usage
    })

df = pd.DataFrame(table_data)
print("\n" + "="*80)
print("TRANSFORMER MODEL ARCHITECTURES AND TRAINING STATISTICS")
print("="*80)
print(df.to_string(index=False))
print("\n" + "="*80)

# Save table
df.to_csv(f"{cfg_main.SAVE_DIR}/model_architectures.csv", index=False)
print(f"✓ Saved table to {cfg_main.SAVE_DIR}/model_architectures.csv")

## Part 3 - Comparative Analysis (Transformer vs LSTM)
"""
PART 3: RNN Scaling Study and Comparison - Detailed Analysis

This cell implements the Part 3 requirements:
1. Compare transformer vs. RNN scaling
2. Determine which architecture scales better (steeper/better exponent)
3. Analyze computational efficiency (training time and memory per parameter)
4. Analyze sample efficiency
5. Provide detailed comparative analysis

Deliverable: "Detailed comparative analysis discussing the scaling behavior differences"
"""

print("\n" + "="*80)
print("PART 3: TRANSFORMER vs LSTM COMPARATIVE ANALYSIS")
print("="*80)

# Load results if needed
if not results:
    try:
        results = json.load(open(f"{cfg_main.SAVE_DIR}/scaling_results.json"))
    except:
        print("Error: No results found. Run Cell 10 and Cell 11 first.")
        results = []

# Separate results by architecture
gpt_results = [r for r in results if r["kind"] == "gpt"]
lstm_results = [r for r in results if r["kind"] == "lstm"]

if not gpt_results or not lstm_results:
    print("Error: Need both GPT and LSTM results. Run Cell 10 and Cell 11 first.")
else:
    # 1. Compare scaling exponents (which architecture scales better?)
    print("\n" + "-"*80)
    print("1. SCALING EXPONENT COMPARISON")
    print("-"*80)

    # Fit power laws for both
    gpt_xs = [r["params"] for r in gpt_results]
    gpt_ys = [r["val_loss"] for r in gpt_results]
    lstm_xs = [r["params"] for r in lstm_results]
    lstm_ys = [r["val_loss"] for r in lstm_results]

    a_gpt, alpha_gpt, c_gpt = fit_power_law(gpt_xs, gpt_ys)
    a_lstm, alpha_lstm, c_lstm = fit_power_law(lstm_xs, lstm_ys)

    print(f"Transformer scaling exponent α = {alpha_gpt:.3f}")
    print(f"LSTM scaling exponent α = {alpha_lstm:.3f}")
    print(f"\nDifference: {abs(alpha_gpt - alpha_lstm):.3f}")

    if alpha_gpt > alpha_lstm:
        print(f"\n✓ Transformer scales BETTER (steeper exponent: {alpha_gpt:.3f} > {alpha_lstm:.3f})")
        print("  → Transformers show better improvement per parameter increase")
    elif alpha_lstm > alpha_gpt:
        print(f"\n✓ LSTM scales BETTER (steeper exponent: {alpha_lstm:.3f} > {alpha_gpt:.3f})")
        print("  → LSTMs show better improvement per parameter increase")
    else:
        print("\n→ Both architectures scale similarly")

    # 2. Computational Efficiency: Time per parameter
    print("\n" + "-"*80)
    print("2. COMPUTATIONAL EFFICIENCY: Training Time per Parameter")
    print("-"*80)

    print("\nTransformer (GPT):")
    for r in gpt_results:
        time_per_param = (r["time_s"] / r["params"]) * 1e6  # seconds per million params
        print(f"  {r['model']:10s} {time_per_param:.4f} sec/M params")

    print("\nLSTM:")
    for r in lstm_results:
        time_per_param = (r["time_s"] / r["params"]) * 1e6
        print(f"  {r['model']:10s} {time_per_param:.4f} sec/M params")

    # Average time per parameter
    avg_gpt_time = np.mean([r["time_s"] / r["params"] for r in gpt_results]) * 1e6
    avg_lstm_time = np.mean([r["time_s"] / r["params"] for r in lstm_results]) * 1e6
    print(f"\nAverage time per parameter:")
    print(f"  Transformer: {avg_gpt_time:.4f} sec/M params")
    print(f"  LSTM:        {avg_lstm_time:.4f} sec/M params")

    if avg_lstm_time < avg_gpt_time:
        print(f"\n✓ LSTM is MORE computationally efficient (faster per parameter)")
        print(f"  → LSTM trains {avg_gpt_time/avg_lstm_time:.2f}x faster per parameter")
    else:
        print(f"\n✓ Transformer is MORE computationally efficient (faster per parameter)")
        print(f"  → Transformer trains {avg_lstm_time/avg_gpt_time:.2f}x faster per parameter")

    # 3. Computational Efficiency: Memory per parameter
    if device == "cuda":
        print("\n" + "-"*80)
        print("3. COMPUTATIONAL EFFICIENCY: GPU Memory per Parameter")
        print("-"*80)

        print("\nTransformer (GPT):")
        for r in gpt_results:
            if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0:
                mem_per_param = (r["gpu_memory_gb"] * 1e9) / r["params"]  # bytes per param
                print(f"  {r['model']:10s} {mem_per_param:.2f} bytes/param ({r['gpu_memory_gb']:.2f} GB total)")

        print("\nLSTM:")
        for r in lstm_results:
            if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0:
                mem_per_param = (r["gpu_memory_gb"] * 1e9) / r["params"]
                print(f"  {r['model']:10s} {mem_per_param:.2f} bytes/param ({r['gpu_memory_gb']:.2f} GB total)")

        # Average memory per parameter
        gpt_mems = [r["gpu_memory_gb"] for r in gpt_results if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0]
        lstm_mems = [r["gpu_memory_gb"] for r in lstm_results if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0]

        if gpt_mems and lstm_mems:
            avg_gpt_mem = np.mean([(m * 1e9) / r["params"] for r, m in zip(gpt_results, gpt_mems) if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0])
            avg_lstm_mem = np.mean([(m * 1e9) / r["params"] for r, m in zip(lstm_results, lstm_mems) if "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0])
            print(f"\nAverage memory per parameter:")
            print(f"  Transformer: {avg_gpt_mem:.2f} bytes/param")
            print(f"  LSTM:        {avg_lstm_mem:.2f} bytes/param")

            if avg_lstm_mem < avg_gpt_mem:
                print(f"\n✓ LSTM uses LESS memory per parameter")
                print(f"  → LSTM is {avg_gpt_mem/avg_lstm_mem:.2f}x more memory efficient")
            else:
                print(f"\n✓ Transformer uses LESS memory per parameter")
                print(f"  → Transformer is {avg_lstm_mem/avg_gpt_mem:.2f}x more memory efficient")
    else:
        print("\n(CPU mode: GPU memory not available)")

    # 4. Sample Efficiency: Validation loss vs training time
    print("\n" + "-"*80)
    print("4. SAMPLE EFFICIENCY: Validation Loss vs Training Time")
    print("-"*80)
    print("(Lower validation loss with same training time = better sample efficiency)")

    # Find models with similar parameter counts for fair comparison
    print("\nComparing models with similar parameter counts:")
    for gpt_r in gpt_results:
        # Find closest LSTM by params
        closest_lstm = min(lstm_results, key=lambda x: abs(x["params"] - gpt_r["params"]))
        param_ratio = closest_lstm["params"] / gpt_r["params"]

        if 0.5 < param_ratio < 2.0:  # Within 2x parameter count
            print(f"\n  ~{gpt_r['params']/1e6:.1f}M params:")
            print(f"    GPT:  val_loss={gpt_r['val_loss']:.3f}, time={gpt_r['time_s']/60:.1f}m")
            print(f"    LSTM: val_loss={closest_lstm['val_loss']:.3f}, time={closest_lstm['time_s']/60:.1f}m")

            if gpt_r['val_loss'] < closest_lstm['val_loss']:
                print(f"    → GPT achieves better loss ({gpt_r['val_loss']:.3f} vs {closest_lstm['val_loss']:.3f})")
            else:
                print(f"    → LSTM achieves better loss ({closest_lstm['val_loss']:.3f} vs {gpt_r['val_loss']:.3f})")

    # 5. Summary and Discussion Points
    print("\n" + "="*80)
    print("5. SUMMARY AND DISCUSSION POINTS FOR REPORT")
    print("="*80)

    print("\nKey Findings:")
    print(f"1. Scaling Exponent:")
    print(f"   - Transformer α = {alpha_gpt:.3f}")
    print(f"   - LSTM α = {alpha_lstm:.3f}")
    if alpha_gpt > alpha_lstm:
        print(f"   - Transformers scale better (steeper improvement with size)")
    else:
        print(f"   - LSTMs scale better (steeper improvement with size)")

    print(f"\n2. Computational Efficiency:")
    print(f"   - Transformer: {avg_gpt_time:.4f} sec/M params average")
    print(f"   - LSTM:        {avg_lstm_time:.4f} sec/M params average")

    if device == "cuda" and gpt_mems and lstm_mems:
        print(f"   - Transformer: {avg_gpt_mem:.2f} bytes/param average")
        print(f"   - LSTM:        {avg_lstm_mem:.2f} bytes/param average")

    print(f"\n3. Why These Differences?")
    print(f"   - Transformers use self-attention (O(n²) complexity)")
    print(f"   - LSTMs use sequential processing (O(n) complexity)")
    print(f"   - Attention allows better long-range dependencies")
    print(f"   - Sequential processing is more memory-efficient")
    print(f"   - Transformers may need more parameters for same capacity")

    print(f"\n4. Sample Efficiency vs Computational Efficiency:")
    print(f"   - Sample efficiency: Which achieves lower loss with same data?")
    print(f"   - Computational efficiency: Which is faster/more memory-efficient?")
    print(f"   - Trade-off: Better scaling may require more compute")

    # Save analysis results
    analysis_results = {
        "scaling_exponents": {
            "transformer": alpha_gpt,
            "lstm": alpha_lstm,
            "better_scaler": "transformer" if alpha_gpt > alpha_lstm else "lstm"
        },
        "time_per_param_avg": {
            "transformer": float(avg_gpt_time),
            "lstm": float(avg_lstm_time),
            "more_efficient": "lstm" if avg_lstm_time < avg_gpt_time else "transformer"
        }
    }

    if device == "cuda" and gpt_mems and lstm_mems:
        analysis_results["memory_per_param_avg"] = {
            "transformer": float(avg_gpt_mem),
            "lstm": float(avg_lstm_mem),
            "more_efficient": "lstm" if avg_lstm_mem < avg_gpt_mem else "transformer"
        }

    json.dump(analysis_results, open(f"{cfg_main.SAVE_DIR}/part3_analysis.json", "w"), indent=2)
    print(f"\n✓ Saved analysis to {cfg_main.SAVE_DIR}/part3_analysis.json")

## Part 3 - Efficiency Plots
"""
PART 3: Efficiency Visualization Plots

Create plots showing computational efficiency metrics for report.
"""

# Plot 1: Training Time per Parameter
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
for kind, marker, color in [("gpt", "o", "blue"), ("lstm", "s", "red")]:
    xs = [r["params"] for r in results if r["kind"] == kind]
    times_per_param = [(r["time_s"] / r["params"]) * 1e6 for r in results if r["kind"] == kind]
    if xs:
        plt.scatter(xs, times_per_param, marker=marker, s=100, label=kind.upper(), color=color, zorder=3)

plt.xscale("log")
plt.xlabel("Model Size (parameters)", fontsize=11)
plt.ylabel("Training Time (sec/M params)", fontsize=11)
plt.title("Computational Efficiency: Time per Parameter", fontsize=12)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)

# Plot 2: Memory per Parameter (if available)
if device == "cuda":
    plt.subplot(1, 2, 2)
    for kind, marker, color in [("gpt", "o", "blue"), ("lstm", "s", "red")]:
        xs = [r["params"] for r in results if r["kind"] == kind and "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0]
        mems_per_param = [(r["gpu_memory_gb"] * 1e9) / r["params"] for r in results if r["kind"] == kind and "gpu_memory_gb" in r and r["gpu_memory_gb"] > 0]
        if xs:
            plt.scatter(xs, mems_per_param, marker=marker, s=100, label=kind.upper(), color=color, zorder=3)

    plt.xscale("log")
    plt.xlabel("Model Size (parameters)", fontsize=11)
    plt.ylabel("GPU Memory (bytes/param)", fontsize=11)
    plt.title("Computational Efficiency: Memory per Parameter", fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)
else:
    plt.subplot(1, 2, 2)
    plt.text(0.5, 0.5, "GPU memory not available\n(CPU mode)",
             ha='center', va='center', fontsize=12)
    plt.axis('off')

plt.tight_layout()
plt.show()

print("Efficiency plots show computational cost per parameter for each architecture.")

## Sample Generation
def load_best(kind="gpt"):
    """Load the best model (lowest validation loss)"""
    best = min([r for r in results if r["kind"] == kind], key=lambda x: x["val_loss"])
    name = best["model"]

    if kind == "gpt":
        cfgm = dict(tcfgs()[[c[0] for c in tcfgs()].index(name)][1])
        model = MiniGPT(vocab_size, **cfgm).to(device)
    else:
        cfgm = dict(lcfgs()[[c[0] for c in lcfgs()].index(name)][1])
        model = MiniLSTM(vocab_size, **cfgm).to(device)

    state = torch.load(f"{cfg_main.SAVE_DIR}/{kind}_{name}.pt", map_location=device)
    model.load_state_dict(state)
    model.eval()
    return model, best

def sample(model, kind, prompt="", max_new=200, temp=1.0, topk=20):
    """Generate text from model"""
    ids = tokenizer.encode(prompt)
    x = torch.tensor([ids], device=device)

    for _ in range(max_new):
        with torch.no_grad():
            logits = model(x)[:, -1, :] if kind == "gpt" else model(x)[:, -1, :]
            logits = logits / temp

            if topk:
                v, idx = torch.topk(logits, k=min(topk, logits.size(-1)))
                logits = torch.full_like(logits, -1e9).scatter(1, idx, v)

            probs = torch.softmax(logits, dim=-1)
            next_id = torch.multinomial(probs, num_samples=1)

        x = torch.cat([x, next_id], dim=1)
        if x.size(1) >= cfg.SEQ_LEN:
            break

    return tokenizer.decode(x[0].tolist())

# Load best model and generate samples
best_model, best_meta = load_best("gpt")
print(f"Best model: {best_meta}")

print("\n" + "="*60)
print("Unconditional Generation:")
print("="*60)
unconditional = sample(best_model, "gpt", prompt="", max_new=256, temp=0.9)
print(unconditional[:500])

print("\n" + "="*60)
print("Conditional Generation (with ABC header):")
print("="*60)
conditional = sample(best_model, "gpt", prompt="X:1\nT:Generated\nM:4/4\nK:C\n", max_new=256, temp=0.9)
print(conditional)

## Convert ABC to MIDI (for evaluation) – seeded from real tunes

from music21 import converter, midi

def abc_to_midi(abc_text, output_path):
    """Convert ABC notation to MIDI file (silent on error)."""
    try:
        score = converter.parse(abc_text, format='abc')
        mf = midi.translate.streamToMidiFile(score)
        mf.open(output_path, 'wb')
        mf.write()
        mf.close()
        return True
    except Exception:
        return False

os.makedirs("./samples", exist_ok=True)

# Build 3 prompts from real tunes: header + first bar
seed_prompts = []
for idx in [0, 1, 2]:  # pick three different tunes
    real_abc = texts[idx]
    lines = real_abc.split("\n")
    seed_lines = []
    for line in lines:
        seed_lines.append(line)
        if line.strip().startswith("|"):  # stop after first bar line
            break
    seed_prompts.append("\n".join(seed_lines) + "\n")

print("Using these seed prompts:\n")
for i, sp in enumerate(seed_prompts, start=1):
    print(f"--- Seed {i} ---")
    print(sp[:300])
    print()

# Generate continuations from these real-seed prompts
samples = [
    sample(best_model, "gpt", prompt=p, max_new=128, temp=0.7, topk=10)
    for p in seed_prompts
]

for i, abc_text in enumerate(samples, start=1):
    abc_path = f"./samples/sample_{i}.abc"
    midi_path = f"./samples/sample_{i}.mid"
    with open(abc_path, "w") as f:
        f.write(abc_text)
    if abc_to_midi(abc_text, midi_path):
        print(f"✓ Saved sample_{i}.abc and sample_{i}.mid")
    else:
        print(f"✗ Failed to convert sample_{i} to MIDI")

## Print Summary Statistics
print("\n" + "="*60)
print("EXPERIMENT SUMMARY")
print("="*60)
print(f"Dataset: {len(texts)} tunes")
print(f"Vocabulary size: {vocab_size}")
print(f"Sequence length: {cfg_main.SEQ_LEN}")
print(f"Batch size (tokens): {cfg_main.BATCH_TOKENS}")

print("\nTransformer Results:")
for r in [r for r in results if r["kind"] == "gpt"]:
    print(f"  {r['model']:10s} {r['params']/1e6:6.2f}M params  val_loss={r['val_loss']:.3f}  time={r['time_s']/60:.1f}m")

print("\nLSTM Results:")
for r in [r for r in results if r["kind"] == "lstm"]:
    print(f"  {r['model']:10s} {r['params']/1e6:6.2f}M params  val_loss={r['val_loss']:.3f}  time={r['time_s']/60:.1f}m")

print(f"\n✓ All results saved to: {cfg_main.SAVE_DIR}/")
print(f"✓ Samples saved to: ./samples/")

"""**Part 4: Best Model Training and Sample Generation (15%)**"""

## Part 4 - Test Perplexity for Best Model
"""
PART 4: Best Model Evaluation - Test Perplexity

Requirements addressed:
- Quantitative metric: Final perplexity on test set
- Uses best model (lowest val loss) from previous training runs
"""

best_model, best_meta = load_best("gpt")
print(f"Loaded best model for test eval: {best_meta}")

def eval_perplexity(loader, model):
    model.eval()
    total_loss, total_tokens = 0, 0
    with torch.no_grad():
        for xb, yb in tqdm(loader, desc="Test", leave=False):
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb)
            loss = nn.functional.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))
            tokens = xb.numel()
            total_loss += loss.item() * tokens
            total_tokens += tokens
    avg_loss = total_loss / total_tokens
    ppl = math.exp(avg_loss)
    return avg_loss, ppl

test_loss, test_ppl = eval_perplexity(test_loader, best_model)
print(f"Test loss: {test_loss:.4f}  |  Test perplexity: {test_ppl:.2f}")

"""
PART 4: Sample Generation & Evaluation

Requirements addressed:
- Generate at least 10 diverse samples (mix of unconditional and conditional)
- Convert generated ABC to MIDI
- Report % syntactically valid ABC and % MIDI success
- Provide files for qualitative analysis and appendix
"""

os.makedirs("./samples_best10", exist_ok=True)

# 10 prompts, all with full ABC headers (L: added to help music21 parsing)
# The first 5 act as "unconditional" seeds with generic titles; last 5 are more specific.
prompts = [
    "X:1\nT:Uncond1\nM:4/4\nL:1/8\nK:C\n",
    "X:2\nT:Uncond2\nM:4/4\nL:1/8\nK:G\n",
    "X:3\nT:Uncond3\nM:4/4\nL:1/8\nK:D\n",
    "X:4\nT:Uncond4\nM:3/4\nL:1/8\nK:F\n",
    "X:5\nT:Uncond5\nM:6/8\nL:1/8\nK:Am\n",
    "X:6\nT:Air\nM:3/4\nL:1/8\nK:G\n",
    "X:7\nT:Reel\nM:4/4\nL:1/8\nK:Dmix\n",
    "X:8\nT:Jig\nM:6/8\nL:1/8\nK:Am\n",
    "X:9\nT:Waltz\nM:3/4\nL:1/8\nK:C\n",
    "X:10\nT:Hornpipe\nM:4/4\nL:1/8\nK:G\n",
]

def looks_valid_abc(txt):
    # Simple heuristic: must contain at least one ABC header
    return any(h in txt for h in ["X:", "T:", "M:", "K:"])

valid_count = 0
midi_success = 0
records = []

for i, p in enumerate(prompts):
    # Slightly lower temp for more stable syntax
    abc_text = sample(best_model, "gpt", prompt=p, max_new=256, temp=0.8, topk=20)
    abc_path = f"./samples_best10/sample_{i+1}.abc"
    midi_path = f"./samples_best10/sample_{i+1}.mid"
    with open(abc_path, "w") as f:
        f.write(abc_text)

    is_valid = looks_valid_abc(abc_text)
    valid_count += int(is_valid)

    converted = abc_to_midi(abc_text, midi_path)
    midi_success += int(converted)

    records.append({
        "idx": i+1,
        "prompt": p,
        "abc": abc_path,
        "midi": midi_path,
        "valid_abc": bool(is_valid),
        "midi_ok": bool(converted),
    })

valid_pct = 100 * valid_count / len(prompts)
midi_pct = 100 * midi_success / len(prompts)
print(f"Validity: {valid_count}/{len(prompts)} = {valid_pct:.1f}%")
print(f"MIDI success: {midi_success}/{len(prompts)} = {midi_pct:.1f}%")

json.dump(records, open("./samples_best10/metadata.json", "w"), indent=2)
print("✓ Saved ABC/MIDI samples and metadata to ./samples_best10")

## Part 4 - Continue Training Best Model for More Tokens
"""
PART 4: Improve the best model with more training

Requirements addressed:
- Train your best model as well as you can within timeline
- Train for more tokens if time permits
- Optionally adjust hyperparameters here (LR, dropout, weight decay, etc.)

Note: This is optional and can be time-consuming. Skip if compute is limited.
"""

# Example: continue training best GPT for one more epoch
best_model, best_meta = load_best("gpt")
optimizer = torch.optim.AdamW(
    best_model.parameters(),
    lr=cfg_main.LR,
    betas=cfg_main.BETAS,
    weight_decay=cfg_main.WEIGHT_DECAY
)

train_loss, val_loss, wall, train_curve, mem_peak = train_one_epoch(
    best_model, optimizer, train_loader, val_loader, track_curve=True
)

torch.save(best_model.state_dict(), f"{cfg_main.SAVE_DIR}/gpt_{best_meta['model']}_continued.pt")
print(f"Continued training best model ({best_meta['model']}):")
print(f"  train={train_loss:.3f} val={val_loss:.3f} time={wall/60:.1f}m  mem={mem_peak:.2f}GB")
print("Saved continued checkpoint:", f"{cfg_main.SAVE_DIR}/gpt_{best_meta['model']}_continued.pt")

"""
FINAL PART 4 SUMMARY: BEST MODEL, TOKENS, TEST METRICS, SAMPLES, HYPERPARAMETERS

This cell aggregates all the key Part 4 information for the report:
- Best transformer architecture and parameter count
- Tokens used (baseline 100M, +100M if continued training was run)
- Continued-training metrics (train/val loss, time, memory)
- Test loss and test perplexity
- Sample validity and MIDI success rates for the 10-sample evaluation
- Hyperparameters used for best-model training / continuation
"""

print("\n" + "="*80)
print("PART 4: BEST MODEL SUMMARY")
print("="*80)

# 1) Best transformer from scaling study (before continuation)
# ------------------------------------------------------------------
if 'results' not in globals() or not results:
    try:
        results = json.load(open(f"{cfg_main.SAVE_DIR}/scaling_results.json"))
    except Exception as e:
        print("Could not load scaling_results.json:", e)
        results = []

gpt_results = [r for r in results if r.get("kind") == "gpt"]
if gpt_results:
    best_meta = min(gpt_results, key=lambda x: x["val_loss"])
    print("\n[Best GPT model from scaling study]")
    print(f"  Model:     {best_meta['model']}")
    print(f"  Params:    {best_meta['params']/1e6:.2f} M")
    print(f"  Val loss:  {best_meta['val_loss']:.3f}  (after 100M tokens)")
else:
    print("\n[Best GPT model]: not available (no GPT results found)")
    best_meta = None

# 2) Tokens used for best model (baseline + continuation)
# ------------------------------------------------------------------
tokens_per_epoch = getattr(cfg_main, "TRAIN_TOKENS_TARGET", 100_000_000)
continued_ckpt_path = None

if best_meta:
    continued_ckpt_path = f"{cfg_main.SAVE_DIR}/gpt_{best_meta['model']}_continued.pt"
    import os
    has_continued = os.path.exists(continued_ckpt_path)
else:
    has_continued = False

print("\n[Token budget for best model]")
if has_continued:
    print(f"  Baseline tokens:  {tokens_per_epoch:,}")
    print(f"  Extra tokens:     {tokens_per_epoch:,}  (continued training)")
    print(f"  Total tokens:     {tokens_per_epoch * 2:,}")
else:
    print(f"  Total tokens:     {tokens_per_epoch:,}  (no continued training run detected)")

# 3) Continued training metrics (if you just ran the continued-training cell)
# ------------------------------------------------------------------
if has_continued and 'train_loss' in globals() and 'val_loss' in globals() and 'wall' in globals() and 'mem_peak' in globals():
    print("\n[Continued training (best GPT model)]")
    print(f"  Checkpoint:   {continued_ckpt_path}")
    print(f"  Train loss:   {train_loss:.3f}")
    print(f"  Val loss:     {val_loss:.3f}")
    print(f"  Time:         {wall/60:.1f} minutes")
    print(f"  Peak GPU mem: {mem_peak:.2f} GB")
else:
    print("\n[Continued training]")
    if has_continued:
        print(f"  Continued checkpoint exists at: {continued_ckpt_path}")
        print("  (Run was done earlier; train/val stats not in current scope.)")
    else:
        print("  Not run or no continued checkpoint found (optional step).")

# 4) Test performance (loss + perplexity)
# ------------------------------------------------------------------
print("\n[Test performance (best GPT model)]")
if 'test_loss' in globals() and 'test_ppl' in globals():
    print(f"  Test loss:       {test_loss:.4f}")
    print(f"  Test perplexity: {test_ppl:.2f}")
else:
    print("  Test metrics not found in current scope. "
          "Re-run the test eval cell (Part 4 - Test Perplexity).")

# 5) Sample validity & MIDI success for best-10 set
# ------------------------------------------------------------------
print("\n[Sample quality: 10 best samples]")
samples_meta_path = "./samples_best10/metadata.json"
if os.path.exists(samples_meta_path):
    try:
        records = json.load(open(samples_meta_path))
        total_samples = len(records)
        valid_count = sum(int(r.get("valid_abc", False)) for r in records)
        midi_success = sum(int(r.get("midi_ok", False)) for r in records)
        valid_pct = 100 * valid_count / total_samples if total_samples else 0.0
        midi_pct = 100 * midi_success / total_samples if total_samples else 0.0

        print(f"  Total samples:           {total_samples}")
        print(f"  Syntactically valid ABC: {valid_count}/{total_samples} = {valid_pct:.1f}%")
        print(f"  MIDI success:            {midi_success}/{total_samples} = {midi_pct:.1f}%")
        print(f"  Samples directory:       ./samples_best10/")
    except Exception as e:
        print("  Could not read samples_best10/metadata.json:", e)
else:
    print("  No samples_best10/metadata.json found. "
          "Run the Part 4 10-sample generation cell first.")

# 6) Hyperparameters used / tuned for best model
# ------------------------------------------------------------------
print("\n[Hyperparameters (best model training & continuation)]")
print(f"  SEQ_LEN:          {cfg_main.SEQ_LEN}")
print(f"  BATCH_TOKENS:     {cfg_main.BATCH_TOKENS}")
print(f"  TRAIN_TOKENS:     {cfg_main.TRAIN_TOKENS_TARGET:,} per epoch")
print(f"  Optimizer:        AdamW")
print(f"  Learning rate:    {cfg_main.LR}")
print(f"  Weight decay:     {cfg_main.WEIGHT_DECAY}")
print(f"  Betas:            {cfg_main.BETAS}")
print(f"  Grad clip norm:   {cfg_main.CLIP_NORM}")
print(f"  Train/val/test:   {100*(1-cfg_main.VAL_FRAC-cfg_main.TEST_FRAC):.1f}% / "
      f"{100*cfg_main.VAL_FRAC:.1f}% / {100*cfg_main.TEST_FRAC:.1f}%")

# If you manually changed LR or weight decay for continuation, print that here too:
if has_continued and 'optimizer' in globals():
    # Attempt to read the LR actually used in the optimizer
    lr_used = optimizer.param_groups[0].get('lr', cfg_main.LR)
    wd_used = optimizer.param_groups[0].get('weight_decay', cfg_main.WEIGHT_DECAY)
    print("\n  [Continuation-specific]")
    print(f"  LR used for continuation:        {lr_used}")
    print(f"  Weight decay for continuation:   {wd_used}")

print("\n" + "="*80)
print("END OF PART 4 SUMMARY")
print("="*80 + "\n")